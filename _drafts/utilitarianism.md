---
layout: post
title: Three Things About Utilitarianism
excerpt: or Contra Caplan Concerning Consequentialism.
---

Epistemic status: seems like the type of thing I might disagree with later, but sharing as a snapshot of my current thinking.

<!--more-->
* table of contents
{: toc }

# Utilitarianism is reasonable

This first point is a vague response to a recent Bryan Caplan interview on, fittingly, the [Utilitarian Podcast](https://forum.effectivealtruism.org/posts/siRv4NLYHS7ySLSZu/podcast-bryan-caplan-on-open-borders-ubi-totalitarianism-ai). Bryan is a libertarian economist, part of the [George Mason Economics Diaspora](https://economics.gmu.edu/people/full_time_faculty) (GMED), and about the most contrarian public figure that I could imagine inviting to a party. I'm generally familiar with his work, and enjoyed his book graphic nonfiction on [Open Borders](https://www.goodreads.com/en/book/show/42867903) (a collaboration with [Zach Weinersmith of SMBC](https://www.smbc-comics.com/)), but haven't read his more controversial [The Myth of the Rational Voter](https://www.goodreads.com/book/show/698866.The_Myth_of_the_Rational_Voter) and [The Case Against Education](https://www.goodreads.com/book/show/36319077-the-case-against-education). You get the idea, though.

Bryan is also _not_ a utilitarian, as he makes very clear in the section of the subsection titled [Objections to utilitarianism](https://forum.effectivealtruism.org/posts/siRv4NLYHS7ySLSZu/podcast-bryan-caplan-on-open-borders-ubi-totalitarianism-ai#Objections_to_utilitarianism). True to [Godwin's law](https://en.wikipedia.org/wiki/Godwin%27s_law), this devolves into a hypothetical about whether or not he's in favor of torturing Hitler (spoiler: he is), but before that, he makes the following points about "the most morally scrupulous utilitarians" he knows:

> And then when I look at those people and then say, Hey, you a person that seems hyper-scrupulous, someone who seems like they would care much more about doing the right thing than about personal convenience or your own money or anything else. And I see that you're not following your own principle.
> 
> That to me is a much better argument against the view, their official view, because this is one where they can't easily say, "yeah, I'm just an evil hypocrite". No, you're not. You're not an evil hypocrite. I've been watching you very carefully and you seem actually you are very, not just thoughtful, but you have a lot of integrity and you're still not doing it.
> 
> You yourself actually don't find the view convincing. And that's why you're not doing it. It's one that you are saying is a moral obligation, but you just haven't really followed through on it because it doesn't seem plausible even to you.

My summary of his argument is something like:

1. I know some genuinely good people who are also utilitarians.
2. Even these people, who I rate as among the best representatives of their stated philosophical views, fall well short of its apparent demands.
3. Therefore they don't _really_ believe that utilitarianism is morally obligatory.

More evidence for this interpretation of Caplan's views can be found in his (undeniably charming) [review of university Effective Altruism groups](https://forum.effectivealtruism.org/posts/czggzS72qsDsCGahg/bryan-caplan-on-ea-groups), where he explicitly compliments the moral inconsistenty of their members:

> Most EAs are official utilitarians. If they were consistent, they’d be Singerian robots who spent every surplus minute helping strangers. But fortunately for me, these self-styled utilitarians severely bend their own rules. In practice, the typical EA is roughly 20% philanthropist, 80% armchair intellectual. They care enough to try make the world a better place, but EA clubs are basically debating societies. Debating societies plus volleyball. That’s utilitarianism I can live with.

First, it seems like the combination of the two quotes puts would-be utilitarians in a bit of a bind. If you _do_ attempt to go Full Utilitarian&trade;, and spend "every surplus minute" reducing the suffering of other sentient beings (at least up to the point where your _own_ suffering dominates), then Caplan dismisses you as a "Singerian robot" (which is probably meant to sound unflattering, but as a robotics engineer...).

On the other hand, if you're a self-styled utilitarian who nevertheless spends 80% of your time debating and playing volleyball, Caplan will say nice things about you, but also accuse you of being philosophically inconsistent. This seems like a lose-lose proposition, with Caplan is updating _against_ utilitarianism regardless of whether its adherents take its apparent demands literally.

But more importantly, _is_ utilitarianism as demanding as Caplan imagines? Are his hyper-scrupulous utilitarian friends really falling short of their moral obligations? I don't think so.

As a self-styled utilitarian, I'd want to know what the _optimal_ amount of selflessness actually _is_ before labeling any particular quantity "what utilitarianism demands." Case in point, Caplan's objections indicate the presence of vocal pushback against the idea of a overly-demanding moral philosophies.

Doing the "obviously utilitarian" thing may be the obligatory action _in a vacuum_, but _signaling_ that utilitarianism is excessively morally demanding seems to turn people off, and this could have large negative consequences _on its own terms_. And as we mentioned above, avoiding negative consequences is kind of the whole point.

Back to Caplan's criticism from the previous section, we'd have to know what the _optimal_ amount of selflessness actually _is_ before labeling anything "utilitarian." Case in point, his objections indicate the presence of vocal pushback against the idea of a overly-demanding moral philosophies. Doing the "obviously" utilitarian thing (e.g. Benthamite calculus, Singerian robotics) may be the correct action _in a vacuum_, but _signaling_ that it excessively morally demanding seems to turn people like Caplan off, which could have even larger negative consequences.

(It almost goes without saying that humans have the tendency to employ unselfish justifications for selfish acts, and so we should be rightfully suspicious of any attempts to explain away apparently suboptimal behavior as a feature [signaling] instead of a bug [inconsistency]. But that trope is so well-established that it has its own dedicated cultural immune response, which is constantly prompting us to search for the selfishness lurking behind unselfish rationalizations. So how to know if we're getting a true or false positive? I guess sometimes you really do just have to crunch the numbers.)

# Utilitarianism is banal

In a recent episode of the excellent Conversations with Tyler podcast, economists Tyler Cowen and Russ Roberts (also members of the George Mason Economics diaspora) also discussed utilitarianism. Roberts had this to say:

> I’m not much of a utilitarian. I think all of us have some utilitarian impulses in the certain cases where it’s overwhelmingly clear that too many people benefit from something, or the size of it’s so large. I think in most of the interesting cases, I think our profession has been so damaged overall by professional economics, by the Benthamite calculus.

Here Roberts seems to be conflating utilitarianism the philosophy with "Benthamite calculus" the action, and potentially for good reason. Jeremy Bentham is widely regarded as the founding father of utilitarianism philosophy, and as Cowen subsequently points out, he was unusually ahead of his time "on issues such as gay rights, or animal welfare, or monetary theory, or usury laws, or tariffs..."

But in another sense, I think Roberts is making a category error. The flow of his statement implies that he's not a utilitarian _because_ he thinks that doing Benthamite calculus has damaged the profession of economics, and that is a bad outcome.

Utilitarians are a diverse group, and I wouldn't presume to speak for all of them, but I can somewhat confidently say that we _do not_ endorse actions that lead to overall bad consequences. This is kind of the whole point. And note that this is true _even if_ that action is "do Benthamite calculus."

Assuming Roberts _is_ making this mistake, why might he be doing so? Perhaps it's because he's "not much of a utilitarian," and so is prone to thinking of moral philosophy from a "what rule does it prescribe?" lens (rather than a more consequentialist "what outcome is it optimizing for?"). This would be a kind of "law of the instrument," nail/hammer error that I suspect is more common than people think.

Another possibility is that stock utilitarianism (as I use the term, at least) is simply too _banal_ to be useful. It can't even be relied upon to recommend seemingly obvious rules like "economists should aggregate utility functions"! In this sense, Roberts might just be saying that we should instead define utilitarianism as something more tangible, like "Benthamite calculus," for the same reason we'd want have a word for horse and not unicorn, if we could only pick one.

Since we _aren't_ limited to only one word, however, I suggest we use _utilitarianism_ to be the thing where we want `max(utility)`, and Benthamite calculus to refer to the act of trying to measure and aggregate utility. As a self-styled _utilitarian_, I think of explicit Benthamite calculus is a sometimes useful tool (especially when we suspect our existing cultural norms are out of their depth), not a mandatory philosophical exercise.

And speaking of terminology, I've been toying with the idea of creating a new browser plugin (working name: #hashedtags&trade;), which uses a hashing algorithm to convert every potentially proper name and slogan into a consistent, indescipherable string in context. The upshot is to force the reader to think about the _content_ of the idea before getting memed by the sometimes evocative language used to identify it.

As a small test, I implemented a version which you can try for yourself on the remainder of the post! Just flip the toggle below:

# Utilitarianism is agnostic

In the same interview with Roberts, Cowen talked briefly about the relationship between utilitarianism and other moral philosophies:

> I would put it this way: Insofar as we can aggregate—and to make a choice, a policy choice, you have to aggregate in some manner—but I think we aggregate by making moral judgements about different kinds of well-being. In this sense, utilitarianism is parasitic on nonutilitarian moral theories about which pleasures and pains we count, and for how much, and even how we understand what pleasure is.

Separately, in a recent post, rationalist blogger and frequent Cowen citee Zvi Moskowitz made a related point:

> I continue to (in my totally-lacking-formal-anything-kind-of-way) view this all largely as ‘if you think your implementation details of [utilitarianism, deontology, virtue ethics] isn’t isomorphic to a valid implementation of either of the other two, your implementation is invalid’ so you can do anything with all three, it’s more that they each make different things easier to see and consider, and point us towards different foci and modes of thought.

I sort of disagree with both, but it's going to take a minute to get around to it.

Despite its recent run-in with the replication crisis, I'll always have a soft spot for behavioral economics, if only because of the mileage I've gotten from the Thinking, Fast and Slow concept of System 1 and 2 thinking. Humans are both feeling _and_ thinking, both sentient _and_ sapient, and whether or not the Systems cleave neurological reality at the joints, they at least provide a clean symbolic framework for approaching ideas about cognition.

And so, when I recently embarked on a yearlong, self-guided (read: rank amateur) effort to get up-to-speed in moral philosophy, I quickly found myself employing Kahneman's framework once again, this time in an attempt to figure out just what the hell Aristotle and Kant were on about. When I think about their respective brands of moral philosophy (what I'm calling virtue ethics and deontology), I see them as efforts to corral human behavior via the Systems, and in a way that is _categorically different_ from consequentialist philosophies like utilitarianism. I'll explain.

In the virtue ethics branch of the dichotomy, we begin with the observation that humans sometimes act selfishly and to determent of others. This happens because our intuitions are misaligned with the greater good, and the solution is to try to cultivate the _correct_ intuitions, called virtues. In my model, this is a System 1 problem, and Aristotle envisions a System 1 solution.

On the deontology side, we start with the _same_ observation and the _same_ diagnosis: humans sometimes act selfishly due their misaligned intuitions. But Kant proposes a different, System 2 solution, which is to imagine the rule that you wish others would follow in your position, and just do it **swish**.

Critically, both Aristotle and Kant stress that you should have a given virtue or follow a given rule _as an end in itself_ [[citation needed]], and _not_ because you are trying to optimize for some other output (i.e. be good for goodness' sake!). Because of this, I use the term _input-focused_ to describe nonutilitarian philosophies like virtue ethics and deontology.

Consequentialist philosophies (like utilitarianism), on the other hand, _exist entirely in output space_. Or put another way, each envisions a desirable _end_ (e.g. `max(utility)`, in the case of "total" utilitarianism), and is roughly _agnostic_ about how we get there (the _input_ side of the equation).

In the above sections, we've already talked about how a utilitarian might endorse being sub-optimally good as an individual (if only to make doing good more appealing to the Caplans of the world), and reject the idea of doing explicit utility aggregation in favor of other measures. Now I claim that utilitarianism _could and often does_ explicitly endorse the same actions recommended by virtue ethics and deontology.

Diagram here? "As a riff on the typical reinforcement learning diagram..."

The reason for this is obvious (and ultimately utilitarian): good virtues and rules have been developed and refined for "optimal" performance on evolutionarily timescales, and it would be foolish to disregard them, for the same reason that nobody sawed off their legs after the invention of the wheel. While we should be wary about _what exactly_ a given virtue or rule was optimized _for_, and ways in which the evolutionary environment differs from the present, _explicit_ utilitarianism (of the kind that Caplan and Roberts reject) still seems most useful on the margins.

So while utilitarianism (and other forms of utilitarianism) may sometimes endorse the recommendations of nonutilitarian philosophies, I think this is different from Cowen's claim that they are parasitic on them _in output space_. In fact, it seems more likely that the reverse is true: why else is a given virtue or rule be _good_ if not because of its _consequences_?

And although I agree with Zvi that a good utilitarian can often find an appropriate virtue ethics or deontological prescription, I don't think the same can be said of a good virtue ethicist or denotologist. Those philosophies are nonutilitarian in part _because_ (at least in my model) they prohibit taking actions because of _output space_ considerations.

If Roberts was making the mistake of seeing utilitarianism through a deontological perspective, then I think Zvi may be doing the reverse, and assuming that, deep down, even nonutilitarians are still consequentialists.

Utilitarianism is only parasitic in the sense that nonutilitarian things like virtues and duties change outcomes for good or bad, or directly make us feel a certain way.

